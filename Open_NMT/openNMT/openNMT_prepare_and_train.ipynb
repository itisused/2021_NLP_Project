{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Sep 30 15:43:02 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.142.00   Driver Version: 450.142.00   CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   81C    P0    70W /  70W |   4260MiB / 15109MiB |     96%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     24718      C   ...rch_latest_p37/bin/python     4257MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (21.2.4)\n",
      "Requirement already satisfied: OpenNMT-py in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (2.1.2)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from OpenNMT-py) (2.6.0)\n",
      "Requirement already satisfied: flask==1.1.2 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from OpenNMT-py) (1.1.2)\n",
      "Requirement already satisfied: pyonmttok<2,>=1.23 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from OpenNMT-py) (1.27.0)\n",
      "Requirement already satisfied: tqdm<5,>=4.51 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from OpenNMT-py) (4.56.0)\n",
      "Requirement already satisfied: torch==1.6.0 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from OpenNMT-py) (1.6.0)\n",
      "Requirement already satisfied: waitress==1.4.4 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from OpenNMT-py) (1.4.4)\n",
      "Requirement already satisfied: pyyaml==5.4 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from OpenNMT-py) (5.4)\n",
      "Requirement already satisfied: torchtext==0.5.0 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from OpenNMT-py) (0.5.0)\n",
      "Requirement already satisfied: configargparse<2,>=1.2.3 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from OpenNMT-py) (1.5.2)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from flask==1.1.2->OpenNMT-py) (1.1.0)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from flask==1.1.2->OpenNMT-py) (2.11.3)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from flask==1.1.2->OpenNMT-py) (1.0.1)\n",
      "Requirement already satisfied: click>=5.1 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from flask==1.1.2->OpenNMT-py) (7.1.2)\n",
      "Requirement already satisfied: future in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from torch==1.6.0->OpenNMT-py) (0.18.2)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from torch==1.6.0->OpenNMT-py) (1.21.2)\n",
      "Requirement already satisfied: six in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from torchtext==0.5.0->OpenNMT-py) (1.15.0)\n",
      "Requirement already satisfied: sentencepiece in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from torchtext==0.5.0->OpenNMT-py) (0.1.96)\n",
      "Requirement already satisfied: requests in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from torchtext==0.5.0->OpenNMT-py) (2.25.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from Jinja2>=2.10.1->flask==1.1.2->OpenNMT-py) (1.1.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from tensorboard<3,>=2.3->OpenNMT-py) (0.36.2)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from tensorboard<3,>=2.3->OpenNMT-py) (1.35.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from tensorboard<3,>=2.3->OpenNMT-py) (3.3.4)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from tensorboard<3,>=2.3->OpenNMT-py) (1.39.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from tensorboard<3,>=2.3->OpenNMT-py) (0.6.1)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from tensorboard<3,>=2.3->OpenNMT-py) (3.17.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from tensorboard<3,>=2.3->OpenNMT-py) (49.6.0.post20210108)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from tensorboard<3,>=2.3->OpenNMT-py) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from tensorboard<3,>=2.3->OpenNMT-py) (1.8.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from tensorboard<3,>=2.3->OpenNMT-py) (0.13.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3->OpenNMT-py) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3->OpenNMT-py) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3->OpenNMT-py) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3->OpenNMT-py) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<3,>=2.3->OpenNMT-py) (2.0.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3->OpenNMT-py) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from requests->torchtext==0.5.0->OpenNMT-py) (1.26.6)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from requests->torchtext==0.5.0->OpenNMT-py) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from requests->torchtext==0.5.0->OpenNMT-py) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from requests->torchtext==0.5.0->OpenNMT-py) (2021.5.30)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3->OpenNMT-py) (3.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<3,>=2.3->OpenNMT-py) (3.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install OpenNMT-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step1: Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus corpus_1's weight should be given. We default it to 1 for you.\n",
      "[2021-09-09 11:20:07,032 INFO] Counter vocab from 32000 samples.\n",
      "[2021-09-09 11:20:07,033 INFO] Build vocab on 32000 transformed examples/corpus.\n",
      "[2021-09-09 11:20:07,039 INFO] corpus_1's transforms: TransformPipe()\n",
      "[2021-09-09 11:20:07,039 INFO] Loading ParallelCorpus(data_notag/src-train-notag.txt, data_notag/tgt-train.txt, align=None)...\n",
      "[2021-09-09 11:20:07,742 INFO] Counters src:38025\n",
      "[2021-09-09 11:20:07,742 INFO] Counters tgt:38434\n"
     ]
    }
   ],
   "source": [
    "!onmt_build_vocab -config kiss.yaml -n_sample -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-09-15 09:08:51,163 INFO] Missing transforms field for corpus_1 data, set to default: [].\n",
      "[2021-09-15 09:08:51,163 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.\n",
      "[2021-09-15 09:08:51,163 INFO] Missing transforms field for valid data, set to default: [].\n",
      "[2021-09-15 09:08:51,163 INFO] Parsed 2 corpora from -data.\n",
      "[2021-09-15 09:08:51,163 INFO] Loading checkpoint from data_notag/run/model_step_50000.pt\n",
      "[2021-09-15 09:08:51,856 INFO] Loading fields from checkpoint...\n",
      "[2021-09-15 09:08:51,856 INFO]  * src vocab size = 38027\n",
      "[2021-09-15 09:08:51,856 INFO]  * tgt vocab size = 38438\n",
      "[2021-09-15 09:08:51,859 INFO] Building model...\n",
      "[2021-09-15 09:08:55,581 INFO] NMTModel(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(38027, 512, padding_idx=1)\n",
      "        )\n",
      "        (pe): PositionalEncoding(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (transformer): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): TransformerEncoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): TransformerEncoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (3): TransformerEncoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(38438, 512, padding_idx=1)\n",
      "        )\n",
      "        (pe): PositionalEncoding(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "    (transformer_layers): ModuleList(\n",
      "      (0): TransformerDecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "        (context_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerDecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "        (context_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerDecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "        (context_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerDecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "        (context_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=38438, bias=True)\n",
      "    (1): Cast()\n",
      "    (2): LogSoftmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "[2021-09-15 09:08:55,583 INFO] encoder: 32080384\n",
      "[2021-09-15 09:08:55,583 INFO] decoder: 56216102\n",
      "[2021-09-15 09:08:55,583 INFO] * number of parameters: 88296486\n",
      "[2021-09-15 09:08:56,220 INFO] Starting training on GPU: [0]\n",
      "[2021-09-15 09:08:56,220 INFO] Start training loop and validate every 1000 steps...\n",
      "[2021-09-15 09:08:56,220 INFO] corpus_1's transforms: TransformPipe()\n",
      "[2021-09-15 09:08:56,221 INFO] Loading ParallelCorpus(data_notag/src-train-notag.txt, data_notag/tgt-train.txt, align=None)...\n",
      "[2021-09-15 09:12:10,601 INFO] Step 50100/100000; acc:  78.57; ppl:  2.24; xent: 0.80; lr: 0.00039; 6030/5506 tok/s;    194 sec\n",
      "[2021-09-15 09:15:25,724 INFO] Step 50200/100000; acc:  76.11; ppl:  2.36; xent: 0.86; lr: 0.00039; 5973/5454 tok/s;    390 sec\n",
      "[2021-09-15 09:18:48,275 INFO] Step 50300/100000; acc:  76.07; ppl:  2.37; xent: 0.86; lr: 0.00039; 5766/5270 tok/s;    592 sec\n",
      "[2021-09-15 09:22:14,904 INFO] Step 50400/100000; acc:  76.32; ppl:  2.35; xent: 0.85; lr: 0.00039; 5666/5166 tok/s;    799 sec\n",
      "[2021-09-15 09:25:30,476 INFO] Step 50500/100000; acc:  76.27; ppl:  2.35; xent: 0.85; lr: 0.00039; 5975/5456 tok/s;    994 sec\n",
      "[2021-09-15 09:28:47,642 INFO] Step 50600/100000; acc:  76.21; ppl:  2.36; xent: 0.86; lr: 0.00039; 5945/5422 tok/s;   1191 sec\n",
      "[2021-09-15 09:32:03,129 INFO] Step 50700/100000; acc:  76.43; ppl:  2.34; xent: 0.85; lr: 0.00039; 5944/5424 tok/s;   1387 sec\n"
     ]
    }
   ],
   "source": [
    "!onmt_train -config kiss.yaml"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "47d517df254fbe59b5c04b69f23871c524fea7fc7dc51e349dcd158d9476f98d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('pytorch_latest_p37': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
