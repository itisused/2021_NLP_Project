{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final Idiom Classifier for KoGPT2 with oversampling.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4CbrivKkd1I"
      },
      "source": [
        "'''\n",
        "pip install --upgrade mxnet>=1.6.0\n",
        "pip install gluonnlp\n",
        "pip install transformers\n",
        "pip install sentencepiece\n",
        "'''\n",
        "\n",
        "import gluonnlp as nlp\n",
        "from gluonnlp.data import SentencepieceTokenizer, SentencepieceDetokenizer\n",
        "from transformers import TFGPT2LMHeadModel\n",
        "import tensorflow as tf\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pickle\n",
        "\n",
        "# oversampling\n",
        "# pip install imblearn\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_F0jJ74kudG"
      },
      "source": [
        "\n",
        "with open('./3/train_34614.pkl','rb') as f:\n",
        "  corpus = pickle.load(f)\n",
        "with open('./3/no_idiom_30000.pkl','rb') as f:\n",
        "  no_idiom = pickle.load(f)\n",
        "\n",
        "corpus = corpus[corpus['Label']==1]\n",
        "\n",
        "new_df = pd.concat([corpus, no_idiom])\n",
        "new_df = new_df.sample(frac=1)\n",
        "new_df.reset_index(drop=True, inplace=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q__7-i7WkwJV"
      },
      "source": [
        "train, test = train_test_split(new_df, test_size=0.2, random_state=42)\n",
        "train, val = train_test_split(train, test_size=0.1, random_state=42)\n",
        "\n",
        "oversample = RandomOverSampler()\n",
        "ko_t = train.ko.to_numpy().reshape(-1, 1)\n",
        "label_t = train.Label.to_numpy().reshape(-1, 1)\n",
        "\n",
        "x_over, y_over = oversample.fit_resample(ko_t, label_t)\n",
        "\n",
        "data = pd.DataFrame({'ko':x_over.reshape(-1)})\n",
        "target = pd.DataFrame({'Label':y_over.reshape(-1)})\n",
        "\n",
        "data = data['ko']\n",
        "target = target['Label']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0-37imEkxGW"
      },
      "source": [
        "dx_train,dx_test, dy_train, dy_test = train_test_split(data, target, test_size=0.2, stratify=target, random_state=42)\n",
        "\n",
        "MY_PATH = '/content/drive/MyDrive/Colab Notebooks/multicampus/Idiom Classifier/4.KoGPT2/'\n",
        "MODEL_PATH = MY_PATH + 'gpt_ckpt'\n",
        "TOKENIZER_PATH = MY_PATH + 'gpt_ckpt/gpt2_kor_tokenizer.spiece'\n",
        "\n",
        "tokenizer = SentencepieceTokenizer(TOKENIZER_PATH, num_best=0, alpha=0)\n",
        "detokenizer = SentencepieceDetokenizer(TOKENIZER_PATH)\n",
        "vocab = nlp.vocab.BERTVocab.from_sentencepiece(TOKENIZER_PATH,\n",
        "                                               mask_token = None,\n",
        "                                               sep_token = None,\n",
        "                                               cls_token = None,\n",
        "                                               unknown_token = '<unk>',\n",
        "                                               padding_token = '<pad>',\n",
        "                                               bos_token = '<s>',\n",
        "                                               eos_token = '</s>')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OD2Pvw0Ykyvl"
      },
      "source": [
        "MAX_LEN = 60\n",
        "def build_data(x_data, y_label):\n",
        "    data_sents = []\n",
        "    data_labels = []\n",
        "\n",
        "    for sent, label in zip(x_data, y_label):\n",
        "        tokenized_text = vocab[tokenizer(sent)]\n",
        "\n",
        "        tokens = [vocab[vocab.bos_token]]\n",
        "        tokens += pad_sequences([tokenized_text], \n",
        "                                MAX_LEN, \n",
        "                                value=vocab[vocab.padding_token], \n",
        "                                padding='post').tolist()[0] \n",
        "        tokens += [vocab[vocab.eos_token]]\n",
        "\n",
        "        data_sents.append(tokens)\n",
        "        data_labels.append(label)\n",
        "\n",
        "    return np.array(data_sents, dtype=np.int64), np.array(data_labels, dtype=np.int64).reshape(-1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "um7WbLkQkzuW"
      },
      "source": [
        "x_train, y_train = build_data(dx_train, dy_train)\n",
        "x_test, y_test = build_data(dx_test, dy_test)\n",
        "\n",
        "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJh8YMo8k2k2"
      },
      "source": [
        "word2idx = {k:v for k, v in vocab.token_to_idx.items()}\n",
        "idx2word = {v:k for k, v in word2idx.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgez95gTk1h-"
      },
      "source": [
        "gpt_model = TFGPT2LMHeadModel.from_pretrained(MODEL_PATH)\n",
        "gpt_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WGcO8sek39W"
      },
      "source": [
        "gpt_model = TFGPT2LMHeadModel.from_pretrained(MODEL_PATH)\n",
        "gpt_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJAgzOiCk5Sn"
      },
      "source": [
        "gpt_model.trainable = True\n",
        "gpt_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_7IRlvFk6Gm"
      },
      "source": [
        "x_input = Input(batch_shape = (None, MAX_LEN + 2), dtype = tf.int32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7S6gWenk8cH"
      },
      "source": [
        "output_gpt = gpt_model(x_input)[0][:, -1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWZeFlQ_k-ko"
      },
      "source": [
        "y_output = Dense(1, activation = 'sigmoid')(output_gpt)\n",
        "model = Model(x_input, y_output)\n",
        "model.compile(loss = 'binary_crossentropy', optimizer = Adam(learning_rate = 2e-5))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIgRFHFFlAGg"
      },
      "source": [
        "hist = model.fit(x_train, y_train, validation_data = (x_test, y_test), epochs=3, batch_size=32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7Ok2mm1lA8J"
      },
      "source": [
        "gpt_model.trainable = False\n",
        "gpt_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifYBHMBplCYh"
      },
      "source": [
        "model = Model(x_input, y_output)\n",
        "model.compile(loss = 'binary_crossentropy', optimizer = Adam(learning_rate = 1e-6))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0CdqQv6lDIZ"
      },
      "source": [
        "hist = model.fit(x_train, y_train, validation_data = (x_test, y_test), epochs=3, batch_size=32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EwlOYH7lOn5"
      },
      "source": [
        "f = open('test_3000.pkl', 'rb')\n",
        "test = pickle.load(f)\n",
        "len(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3ovuse4lQ16"
      },
      "source": [
        "text, _ = build_data(test['ko'], np.zeros(len(test)))\n",
        "\n",
        "pred = model.predict(text)\n",
        "y_pred = np.where(pred > 0.5, 1, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNJg1ZPilTaS"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\n",
        "\n",
        "print(\"accuracy:\", accuracy_score(label, y_pred))\n",
        "print(\"precision:\", precision_score(label, y_pred))\n",
        "print(\"recall:\", recall_score(label, y_pred))\n",
        "print(\"F1-Score:\", f1_score(label, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}